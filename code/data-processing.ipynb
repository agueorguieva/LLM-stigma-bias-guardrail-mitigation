{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93f94f6-3b76-4fc3-b76d-fb65ab346324",
   "metadata": {},
   "source": [
    "## Necessary Data Processing ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0faa6cf5-903e-45cc-959b-3531585ef18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362dcb47-10a7-467b-affa-70653debacdc",
   "metadata": {},
   "source": [
    "#### Processing for Experiment 3: percent biases and guardrail model performance ####\n",
    "\n",
    "Creating dataframes that identify, per model, how much bias was decreased after identifying inputs as harmful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e983ff90-c258-4f1b-8b88-4b1e58cf6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the df from IBM that includes all prompts, stigmas, what is a biased answer and cluster types\n",
    "df_ssqa = pd.read_csv(\"hf://datasets/ibm/SocialStigmaQA/yes_no/full_dataset_yes_no.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ed446b6-b234-4fbd-b610-a2e97d052975",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in harm detection results from guardrail models\n",
    "granite_bias_detection = pd.read_csv('/data/raw-guardrail-model-results/guardian-bias-detection-results.csv')\n",
    "granite_harm_detection = pd.read_csv('/data/raw-guardrail-model-results/guardian-harm-detection-results.csv')\n",
    "llama_guard = pd.read_csv('/data/raw-guardrail-model-results/llama_guard_all.csv')\n",
    "mistral_moderation = pd.read_csv('/data/raw-guardrail-model-results/mistral-moderation-detection-results.csv')\n",
    "\n",
    "#load in ssqa (socialstigmaqa) performance\n",
    "SSQA = pd.read_csv(\"/data/SSQA-results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "878211a8-1822-4d69-8637-0a9dfdc45a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "granite_reset = SSQA[SSQA[\"model\"] == \"granite\"].reset_index()\n",
    "mistral_reset = SSQA[SSQA[\"model\"] == \"mistral\"].reset_index()\n",
    "llama_reset =  SSQA[SSQA[\"model\"] == \"llama\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "bfab7309-c3c3-45fd-aa51-8973eadeeb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that returns post-guardrail bias (len = 10360)\n",
    "\n",
    "def post_guardrail_bias(table, guardrail_results_table, guardrail_results_column, bias_value):\n",
    "    post_bias = []\n",
    "    for i in range(0, len(granite_reset)):\n",
    "        #did granite give a biased answer?\n",
    "        if table[\"biased answer\"][i] == 1:\n",
    "            #if it did, then check if the guardrail flagged it as harmful\n",
    "            if guardrail_results_table[guardrail_results_column][i] == bias_value:\n",
    "                #if it did, change to unbiased\n",
    "                post_bias.append(0)\n",
    "            else:\n",
    "                #else, keep it biased\n",
    "                post_bias.append(1)\n",
    "        else:\n",
    "            post_bias.append(0)\n",
    "    return post_bias\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b225bb85-484b-449e-b056-2ca8b655d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for harm default:\n",
    "post_granite_bias = post_guardrail_bias(granite_reset, granite_harm_detection, \"risk of harm (label)?\", \"Yes\")\n",
    "\n",
    "#for social bias category: \n",
    "post_granite_bias_bias = post_guardrail_bias(granite_reset, granite_bias_detection, \"risk of social bias (label)?\", \"Yes\")                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "6ded075f-37e1-4627-b0ce-10e5eac0ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "granite_and_granite_guardian_data = {\"stigma\": df_ssqa['stigma'], \"prompt\": df_ssqa['prompt'], \"prompt style\": df_ssqa['prompt style'],\n",
    "                               \"biased answer\" : df_ssqa['biased answer'], \"risk of harm (label)?\" : granite_harm_detection[\"risk of harm (label)?\"],\n",
    "                               \"probability of harm?\" : granite_harm_detection[\"probability of harm?\"],\n",
    "                               \"original granite answer\": granite_reset[\"model answer\"], \"original answer bias\": granite_reset['biased answer'],\n",
    "                               \"post guardian answer bias\" : post_granite_bias, \"clusters\": granite_reset['clusters'],\n",
    "                                \"risk of social bias (label)?\": granite_bias_detection['risk of social bias (label)?'], \t\n",
    "                                \"probability of social bias?\": granite_bias_detection['probability of social bias?'],\n",
    "                        \"post bias guardian answer bias\": post_granite_bias_bias}\n",
    "granite_and_granite_guardian = pd.DataFrame(granite_and_granite_guardian_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "55bdcb48-77d7-413c-99d6-9bf592d88ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_and_mistral_moderation_data = mistral_moderation.copy()\n",
    "mistral_and_mistral_moderation_data['post moderation answer bias'] = post_guardrail_bias(mistral_reset, mistral_moderation, \"any_true_threshold\", True)\n",
    "mistral_and_mistral_moderation_data['cluster names'] = mistral_reset['clusters']\n",
    "mistral_and_mistral_moderation_data ['original answer bias'] = mistral_reset['biased answer']\n",
    "mistral_and_mistral_moderation = pd.DataFrame(mistral_and_mistral_moderation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1dc5782b-8ed9-4601-91d4-fe3539741a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifically for llama since it needs to != safe\n",
    "post_bias_llama = []\n",
    "for i in range(0, len(llama_reset)):\n",
    "        #did granite give a biased answer?\n",
    "    if llama_reset[\"biased answer\"][i] == 1:\n",
    "            #if it did, then check if the guardrail flagged it as harmful\n",
    "        if llama_guard['safe label'][i] != \"\\n\\nsafe\":\n",
    "                #if it did, change to unbiased\n",
    "                post_bias_llama.append(0)\n",
    "        else:\n",
    "                #else, keep it biased\n",
    "                post_bias_llama.append(1)\n",
    "    else:\n",
    "            post_bias_llama.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "db9692f7-c304-4a0a-a321-0e2e9bbc1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsafe_category = []\n",
    "\n",
    "for i in llama_guard['safe label']:\n",
    "    if i != \"\\n\\nsafe\":\n",
    "        unsafe_category.append(i[-2:])\n",
    "    else:\n",
    "        unsafe_category.append('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "1889267f-6f51-4eb9-8338-a186e0b3af32",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_and_llama_guard_data = {\"stigma\": df_ssqa['stigma'], \"prompt\": df_ssqa['prompt'], \"prompt style\": df_ssqa['prompt style'],\n",
    "                               \"biased answer\" : df_ssqa['biased answer'], \n",
    "                               \"original llama answer\": llama_reset[\"model answer\"], \"original answer bias\" : llama_reset['biased answer'],\n",
    "                              \"llama guard safety label\" : llama_guard['safe label'], \"llama guard unsafe category\": unsafe_category,\n",
    "                               \"post llama answer bias\" : post_bias_llama, \"clusters\": llama_reset['clusters']}\n",
    "llama_and_llama_guard = pd.DataFrame(llama_and_llama_guard_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "dffe7f27-2cd7-452e-807b-663c52a7b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "granite_and_granite_guardian.to_csv('granite_and_granite_guardian.csv')\n",
    "mistral_and_mistral_moderation.to_csv('mistral_and_mistral_moderation.csv')\n",
    "llama_and_llama_guard.to_csv('llama_and_llama_guard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaaf51c-22cc-4b94-b0be-3bca8dd35d5d",
   "metadata": {},
   "source": [
    "### Getting percentages of biased answers per stigma for Experiment 2 and 3 analysis ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ab41a8-78fc-4c36-bdf6-f361d670f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#granite = pd.read_csv(\"/data/SSQA-performance-and-guardrail-mitigations/mistral_and_mistral_moderation.csv\")\n",
    "#llama = pd.read_csv(\"/data/SSQA-performance-and-guardrail-mitigations/llama_and_llama_guard.csv\")\n",
    "#mistral = pd.read_csv(\"/data/SSQA-performance-and-guardrail-mitigations/mistral_and_mistral_moderation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "c3383c2d-10b9-4cca-aabd-f4ab7d5ec2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pachankis_all = pd.read_csv('/data/results-from-pachankis-all.csv')\n",
    "#save each model seperate\n",
    "llama_data = {\"Stigma\": pachankis_all['Stigma'],\"Cluster\": pachankis_all['Cluster'], \"Visibility\" : pachankis_all['visibility-llama'], \"Persistent Course\": pachankis_all['course-llama'],\n",
    "             \"Disrupt\": pachankis_all['disrupt-llama'], \"Unappealing Aesthetics\" : pachankis_all['aesthetics-llama'],\n",
    "       \"Controllable Origin\": pachankis_all['origin-llama'], \"Peril\" :pachankis_all['peril-llama'], \n",
    "              \"visibility human\": pachankis_all['visibility-participants'], \"course human\": pachankis_all['course-participants'], \n",
    "               \"disrupt human\": pachankis_all['disrupt-participants'],\"aesthetics human\": pachankis_all['aesthetics-participants'],\n",
    "               \"origin human\":pachankis_all['origin-participants'], \"peril human\": pachankis_all['peril-participants']}\n",
    "llama_percentages = pd.DataFrame(llama_data)\n",
    "\n",
    "mistral_data = {\"Stigma\": pachankis_all['Stigma'],\"Cluster\": pachankis_all['Cluster'], \"Visibility\" : pachankis_all['visibility-mistral'], \"Persistent Course\": pachankis_all['course-mistral'],\n",
    "             \"Disrupt\": pachankis_all['disrupt-mistral'], \"Unappealing Aesthetics\" : pachankis_all['aesthetics-mistral'],\n",
    "       \"Controllable Origin\": pachankis_all['origin-mistral'], \"Peril\" :pachankis_all['peril-mistral'], \n",
    "              \"visibility human\": pachankis_all['visibility-participants'], \"course human\": pachankis_all['course-participants'], \n",
    "               \"disrupt human\": pachankis_all['disrupt-participants'],\"aesthetics human\": pachankis_all['aesthetics-participants'],\n",
    "               \"origin human\":pachankis_all['origin-participants'], \"peril human\": pachankis_all['peril-participants']}\n",
    "mistral_percentages = pd.DataFrame(mistral_data)\n",
    "\n",
    "granite_data = {\"Stigma\": pachankis_all['Stigma'],\"Cluster\": pachankis_all['Cluster'], \"Visibility\" : pachankis_all['visibility-granite'], \"Persistent Course\": pachankis_all['course-granite'],\n",
    "             \"Disrupt\": pachankis_all['disrupt-granite'], \"Unappealing Aesthetics\" : pachankis_all['aesthetics-granite'],\n",
    "       \"Controllable Origin\": pachankis_all['origin-granite'], \"Peril\" :pachankis_all['peril-granite'], \n",
    "              \"visibility human\": pachankis_all['visibility-participants'], \"course human\": pachankis_all['course-participants'], \n",
    "               \"disrupt human\": pachankis_all['disrupt-participants'],\"aesthetics human\": pachankis_all['aesthetics-participants'],\n",
    "               \"origin human\":pachankis_all['origin-participants'], \"peril human\": pachankis_all['peril-participants']}\n",
    "granite_percentages = pd.DataFrame(granite_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "e5786612-5b4c-4658-b77f-fda2c58d3b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get averages per stigma\n",
    "def get_bias_per_stigma(table, post_bias_column):\n",
    "    percent_pre = []\n",
    "    percent_post = []\n",
    "    for stigma in pachankis_all['Stigma']:\n",
    "        percent_pre.append(sum(table[table['stigma'] == stigma]['original answer bias']) / 111)\n",
    "        percent_post.append(sum(table[table['stigma'] == stigma][post_bias_column]) / 111)\n",
    "    return percent_pre, percent_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "e259c57b-82a2-4502-b742-718e8ea7b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_llama, post_llama = get_bias_per_stigma(llama_and_llama_guard, \"post llama answer bias\")\n",
    "pre_mistral, post_mistral = get_bias_per_stigma(mistral_and_mistral_moderation, \"post moderation answer bias\")\n",
    "pre_granite, post_granite =  get_bias_per_stigma(granite_and_granite_guardian, \"post guardian answer bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "3be12724-37c7-490e-8c9e-54fe0d85b5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_percentages['percent biased'] = pre_llama\n",
    "llama_percentages['percent biased post guardian'] = post_llama\n",
    "mistral_percentages['percent biased'] = pre_mistral\n",
    "mistral_percentages['percent biased post guardian'] = post_mistral\n",
    "granite_percentages['percent biased'] = pre_granite\n",
    "granite_percentages['percent biased post guardian'] = post_granite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "5b4bba8b-49b6-468b-a412-4bda67223e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llama_percentages.to_csv('llama_bias_percentages.csv')\n",
    "#mistral_percentages.to_csv('mistral_bias_percentages.csv')\n",
    "#granite_percentages.to_csv('granite_bias_percentages.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0279aaf6-d169-43e9-b1a9-95df6c05cbdb",
   "metadata": {},
   "source": [
    "Reformatting percent bias changes for cluster and prompt style for easier visualization in R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4121e3-d422-45f0-bda1-f82a263fbfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_guard = pd.read_csv(\"mistral_and_mistral_moderation.csv\")\n",
    "llama_guard = pd.read_csv(\"llama_and_llama_guard.csv\")\n",
    "granite_guard = pd.read_csv(\"granite_and_granite_guardian.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "137f02a7-5165-400f-b0c6-fdcc8af25d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_bias_per_cluster(table, pre_or_post):\n",
    "    none = sum(table[table[\"clusters\"] == \"no stigma\"][pre_or_post]) / len(table[table[\"clusters\"] == \"no stigma\"])\n",
    "    one = sum(table[table[\"clusters\"] == \"1\"][pre_or_post]) / len(table[table[\"clusters\"] == \"1\"])\n",
    "    two = sum(table[table[\"clusters\"] == \"2\"][pre_or_post]) / len(table[table[\"clusters\"] == \"2\"])\n",
    "    three = sum(table[table[\"clusters\"] == \"3\"][pre_or_post]) / len(table[table[\"clusters\"] == \"3\"])\n",
    "    four = sum(table[table[\"clusters\"] == \"4\"][pre_or_post]) / len(table[table[\"clusters\"] == \"4\"])\n",
    "    five = sum(table[table[\"clusters\"] == \"5\"][pre_or_post]) / len(table[table[\"clusters\"] == \"5\"])\n",
    "            \n",
    "    return none, one, two, three, four, five\n",
    "\n",
    "def percent_bias_per_prompt_style(table, pre_or_post):\n",
    "    base = sum(table[table[\"prompt style\"] == \"base\"][pre_or_post]) / len(table[table[\"prompt style\"] == \"base\"])\n",
    "    original = sum(table[table[\"prompt style\"] == \"original\"][pre_or_post]) / len(table[table[\"prompt style\"] == \"original\"])\n",
    "    positive = sum(table[table[\"prompt style\"] == \"positive\"][pre_or_post]) / len(table[table[\"prompt style\"] == \"positive\"])\n",
    "    doubt = sum(table[table[\"prompt style\"] == \"doubt\"][pre_or_post]) / len(table[table[\"prompt style\"] == \"doubt\"])\n",
    "\n",
    "    return base, original, positive, doubt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911341b-b331-43cf-8bdb-da915dbc3ddf",
   "metadata": {},
   "source": [
    "Processing data to make change in bias per cluster type easier to graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030c9ae-4019-4e55-8e2c-b6ae7f7b2e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_pre_llama = percent_bias_per_cluster(llama_guard, \"original answer bias\")\n",
    "bias_pre_mistral = percent_bias_per_cluster(mistral_guard, \"original answer bias\")\n",
    "bias_pre_granite = percent_bias_per_cluster(granite_guard, \"original answer bias\")\n",
    "bias_pre_granite_1 = percent_bias_per_cluster(granite_guard, \"original answer bias\")\n",
    "\n",
    "bias_post_llama = percent_bias_per_cluster(llama_guard, \"post llama guard answer bias\")\n",
    "bias_post_mistral = percent_bias_per_cluster(mistral_guard, \"post moderation answer bias\")\n",
    "bias_post_granite = percent_bias_per_cluster(granite_guard, \"post guardian answer bias\")\n",
    "bias_post_granite_bias_guardian = percent_bias_per_cluster(granite_guard, \"post bias guardian answer bias\")\n",
    "\n",
    "all_tuples = [bias_pre_llama, bias_post_llama, bias_pre_mistral, bias_post_mistral, bias_pre_granite, bias_post_granite, bias_pre_granite_1, bias_post_granite_bias_guardian]\n",
    "bias_pre_and_post_all = list(itertools.chain(*all_tuples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0940d36-24fe-4acb-8181-e697e4951076",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_1 = [[\"llama\"] * 12, [\"mistral\"] * 12, [\"granite\"] * 12, [\"granite bias guardian\"] * 12]\n",
    "models = list(itertools.chain.from_iterable(models_1))\n",
    "clusters = [\"No stigma\", \"Awkward\", \"Threatening\", \"Sociodemographic\",\"Innocuous Persistent\", \"Unappealing Persistent\"] * 8\n",
    "pre_post = [\"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\", \"B\"] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98fe62-748d-4c2a-845f-f2c454b120d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_graphing = {\"cluster\": clusters, 'percent bias per cluster' : bias_pre_and_post_all, \"model\": models, \"type\":pre_post}\n",
    "rq3_table = pd.DataFrame(data_for_graphing)\n",
    "rq3_table.to_csv(\"rq3_data_for_bias_change.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2910c1a-0552-4d34-8845-017206e902a5",
   "metadata": {},
   "source": [
    "Processing data to make change in bias per prompt style easier to graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bd12bb-ac3d-499d-9a1c-12eaf8b5cdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_pre_llama_promptstyle = percent_bias_per_prompt_style(llama_guard, \"original answer bias\")\n",
    "bias_pre_mistral_promptstyle = percent_bias_per_prompt_style(mistral_guard, \"original answer bias\")\n",
    "bias_pre_granite_promptstyle = percent_bias_per_prompt_style(granite_guard, \"original answer bias\")\n",
    "bias_pre_granite_promptstyle_1 = percent_bias_per_prompt_style(granite_guard, \"original answer bias\")\n",
    "\n",
    "bias_post_llama_promptstyle = percent_bias_per_prompt_style(llama_guard, \"post llama guard answer bias\")\n",
    "bias_post_mistral_promptstyle = percent_bias_per_prompt_style(mistral_guard, \"post moderation answer bias\")\n",
    "bias_post_granite_promptstyle = percent_bias_per_prompt_style(granite_guard, \"post guardian answer bias\")\n",
    "bias_post_bias_granite_promptstyle = percent_bias_per_prompt_style(granite_guard, \"post bias guardian answer bias\")\n",
    "\n",
    "all_tuples_promptstyle = [bias_pre_llama_promptstyle, bias_post_llama_promptstyle, bias_pre_mistral_promptstyle, bias_post_mistral_promptstyle, bias_pre_granite_promptstyle, bias_post_granite_promptstyle, bias_pre_granite_promptstyle_1, bias_post_bias_granite_promptstyle]\n",
    "bias_pre_and_post_all_promptstyle = list(itertools.chain(*all_tuples_promptstyle))\n",
    "\n",
    "models_1_promptstyle = [[\"llama\"] * 8, [\"mistral\"] * 8, [\"granite\"] * 8 , [\"granite bias guardian\"] * 8]\n",
    "models_promptstyle = list(itertools.chain.from_iterable(models_1_promptstyle))\n",
    "pre_post_promptstyle = [\"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\"] * 4\n",
    "promptstyle = [\"base\", \"original\", \"positive\", \"doubt\"] * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd376aef-a722-42d7-92db-c58a516e1362",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_prompt_style = {'prompt style': promptstyle, 'percent bias per style' : bias_pre_and_post_all_promptstyle, \"model\": models_promptstyle, \"type\":pre_post_promptstyle}\n",
    "promptstyle_data = pd.DataFrame(data_for_prompt_style)\n",
    "promptstyle_data.to_csv(\"rq3_data_for_prompt_style.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
